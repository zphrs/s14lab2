{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 3 Advanced"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model\n",
    "\n",
    "Linear model: $y_i=\\beta_0+\\beta_1 x_{i1}+\\dots+\\beta_p x_{ip}+\\epsilon$ for $i=1,\\dots,n$\n",
    "\n",
    "Matrix notation: $y=X\\beta+\\epsilon$\n",
    "\n",
    "- $y$ - response variable\n",
    "- $x_1, \\dots, x_p$ - set of $p$ regressors\n",
    "- $\\epsilon$ - noise\n",
    "\n",
    "Approximation: $\\hat{y}=X\\hat{\\beta}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ordinary Least Squares\n",
    "\n",
    "Goal: Find values for $\\beta$ that \"best\" fit the data\n",
    "\n",
    "Question: What's the definition of \"best\"?\n",
    "\n",
    "Error in predictions is the difference between the actual value and the predicted value: $y-\\hat{y}$\n",
    "![Image](https://i1.wp.com/statisticsbyjim.com/wp-content/uploads/2017/04/residuals.png?resize=300%2C186&ssl=1)\n",
    "\n",
    "Squaring the difference accounts for overprediction and underprediction: $(y-\\hat{y})^2$\n",
    "![image](https://miro.medium.com/max/628/1*uBnjPy5o59FfkkMEJL0Nqw.jpeg)\n",
    "\n",
    "The motivation behind OLS is minimizing the sum of squared errors: $\\hat{\\beta} = \\underset{\\beta}{\\operatorname{argmin}} ||y-\\hat{y}||_2^2$\n",
    "\n",
    "OLS is BLUE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assumptions of OLS\n",
    "\n",
    "- Linearity - $E[y]=X\\beta$\n",
    "- Strict exogeneity - $E[\\epsilon|X]=0$\n",
    "- No perfect multicollinearity - Regressors can't be linearly dependent, X has full rank, $\\Pr[\\text{rank}(X)=p]=1$\n",
    "- Independent errors\n",
    "- Homoscedasticity - $E[\\epsilon_i^2|X]=\\sigma^2$\n",
    "- No autocorrelation - $E[\\epsilon_i\\epsilon_j|X]=0$ for $i\\neq j$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solving OLS\n",
    "\n",
    "Goal: Minimize our objective $||y-\\hat{y}||_2^2$\n",
    "\n",
    "Idea: Take the derivative, set equal to 0, and solve for $\\hat{\\beta}$\n",
    "\n",
    "\\begin{align*}\n",
    "||y-\\hat{y}||_2^2 &= (y-\\hat{y})^T (y-\\hat{y}) \\\\\n",
    "&= (y-X\\hat{\\beta})^T (y-X\\hat{\\beta}) \\\\\n",
    "&= y^Ty - \\hat{\\beta}^TX^ty - y^TX\\hat{\\beta} + \\hat{\\beta}^TX^TX\\hat{\\beta} \\\\\n",
    "&= y^Ty - 2\\hat{\\beta}^TX^ty + \\hat{\\beta}^TX^TX\\hat{\\beta} \\\\\n",
    "\\\\\n",
    "\\nabla_\\beta ||y-\\hat{y}||_2^2 &= -2X^Ty + 2X^TX\\hat{\\beta} \\\\\n",
    "\\\\\n",
    "-2X^Ty + 2X^TX\\hat{\\beta} &= 0 \\\\\n",
    "\\Rightarrow X^TX\\hat{\\beta} &= X^Ty \\\\\n",
    "\\Rightarrow \\hat{\\beta} &= (X^TX)^{-1}X^Ty\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn as sk\n",
    "import sklearn.linear_model\n",
    "import sklearn.preprocessing\n",
    "import sklearn.model_selection\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing OLS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate a dataset\n",
    "\n",
    "Generate 1000 samples of $X_1 \\sim U(0, 10)$\n",
    "\n",
    "Generate 1000 samples of $X_2 \\sim U(-20, -10)$\n",
    "\n",
    "Generate $y = 2 + 10X_1 - 5X_2 + \\epsilon$ where $\\epsilon \\sim N(0, 1)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solve for $\\beta$\n",
    "\n",
    "1. Use the closed form solution derived above (HINT: you need to add a column of ones)\n",
    "\n",
    "2. Use sklearn's linear regression\n",
    "\n",
    "Check that estimated coefficients from methods 1 and 2 are the same and match the data generating process.\n",
    "\n",
    "Why do we need to add a column of ones for method 1? What happens if we don't add any noise ($\\epsilon$) to our data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert answers here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regularization helps prevent overfitting and increases the generalizability of your model. By adding regularization, we are reducing variance at the expense of bias in our model. The two most common forms of regularization for linear regression are ridge regression and LASSO. These two regularization methods add a L2 or L1 penalty term to the objective, respectively.\n",
    "\n",
    "Ridge regression: $||y-\\hat{y}||_2^2 + \\lambda||\\beta||_2^2$\n",
    "\n",
    "LASSO: $||y-\\hat{y}||_2^2 + \\lambda||\\beta||_1$\n",
    "\n",
    "Ridge regression is able to shrink all coefficients towards 0 while LASSO can set some coefficients towards 0. Because of this, LASSO is also able to perform feature selection. The last section of this blog post explains this concept visually. https://towardsdatascience.com/ridge-and-lasso-regression-a-complete-guide-with-python-scikit-learn-e20e34bcbf0b\n",
    "\n",
    "In order to effectively use regularization, all the regressors ($X$) must be standardized so coefficients can be compared with each other and penalized accordingly. For both of these methods, the regularization parameters needs to be tuned."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why does standardization change the coefficients but not the $R^2$ score of OLS?\n",
    "\n",
    "Why does standarization change the coefficients and the $R^2$ score for LASSO?\n",
    "\n",
    "Derive the closed form solution to ridge regression (HINT: you should get $\\hat{\\beta}=(X^TX+\\lambda I)^{-1}X^Ty$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert answers here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert derivation here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Effects of collinearity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the dataset from the previous section, add another variable $X_3 = .5 X_1 + .5 X_2 + \\epsilon$ where $\\epsilon \\sim N(0, 0.1)$\n",
    "\n",
    "Generate $y = 2 + 10X_1 - 5X_2 + 7X_3 + \\epsilon$ where $\\epsilon \\sim N(0, 1)$\n",
    "\n",
    "Do not standardize your variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use OLS to estimate the coefficients. How close are they to $\\beta$?\n",
    "\n",
    "Use ridge regression with the default hyperparameters to estimate the coefficients. How close are they to $\\beta$?\n",
    "\n",
    "Use LASSO with the default hyperparameters to estimate the coefficients. How close are they to $\\beta$?\n",
    "\n",
    "Explain the behavior of these three methods.\n",
    "\n",
    "What would happen if $X_3 = .5X_1 + .5X_2$?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert answers here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image sources\n",
    "- https://statisticsbyjim.com/glossary/ordinary-least-squares/\n",
    "- https://medium.com/@saahil1292/machine-learning-102-linear-regression-ordinary-least-squares-ols-correlation-and-analysis-of-7d53751ea9f4"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
